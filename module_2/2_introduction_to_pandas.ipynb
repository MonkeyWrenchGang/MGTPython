{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MonkeyWrenchGang/MGTPython/blob/main/module_2/2_introduction_to_pandas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s98x4IFY4nnd"
      },
      "source": [
        "# Introduction to Pandas \n",
        "---\n",
        "\n",
        "Pandas is a library in Python that is used for data manipulation and analysis. It is a powerful tool that provides easy-to-use data structures and data analysis tools for handling and manipulating large amounts of data. You can think of it as a spreadsheet with rows and columns that you can manipulate and analyze using Python.\n",
        "\n",
        "\n",
        "In this tutoral we'll cover the following Pandas basics\n",
        "\n",
        "1. Reading CSV & Excel Files \n",
        "2. Pandas Properties \n",
        "3. Data Types \n",
        "    - converting data types \n",
        "4. Filtering Columns \n",
        "5. Filtering Rows \n",
        "6. Filtering Rows using Queries\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Libraries"
      ],
      "metadata": {
        "id": "vtd7Ztwl5AC0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wg90iGYU4nng"
      },
      "outputs": [],
      "source": [
        "# -- basic stuff for your notebook -- \n",
        "from IPython.core.display import display, HTML\n",
        "from IPython.display import clear_output\n",
        "display(HTML(\"<style>.container { width:90% }</style>\"))\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "# ------------------------------------------------------------------\n",
        "\n",
        "# -- core packages --\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# -- need this to render charts in notebook -- \n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48rgVwCV4nni"
      },
      "source": [
        "## 1. Reading CSV & Excel Files \n",
        "\n",
        "Here we are going to read the following CSV and Excel files into data frames, we'll also **eyeball** the data \n",
        "\n",
        "- appl.csv    : a comma delimited file of dailiy prices for Apple\n",
        "- msft.xlsx   : an excel file of daily prices for Microsoft \n",
        "- churn.csv   : a comma delimited file of telco churn \n",
        "- titanic.csv : a comma delimited file of titanic survivors and not-survivor \n",
        "\n",
        "to do this we'll use the `dataframe = read_*(\"file location\")` template then we'll check our data with `dataframe.head()`\n",
        "\n",
        "```python\n",
        "aapl = pd.read_csv(\"data/aapl.csv\")\n",
        "aapl.head()\n",
        "\n",
        "msft = pd.read_excel(\"data/msft.xlsx\")\n",
        "msft.head()\n",
        "\n",
        "churn = pd.read_csv(\"data/churn.csv\")\n",
        "churn.head()\n",
        "\n",
        "titanic = pd.read_csv(\"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\")\n",
        "titanic.head()\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mtf2vbj04nni"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sMgGFT9J4nnj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SqcuewHI4nnj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tLrZ5A2S4nnj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EETOViNF4nnj"
      },
      "source": [
        "## 2. Pandas Properties \n",
        "\n",
        "https://pandas.pydata.org/pandas-docs/stable/reference/frame.html\n",
        "\n",
        "- index: returns a sequence used for indexing and alignment. index (row labels) of the dataframe \n",
        "- shape: returns the number of rows and columns of a data frame. \n",
        "- columns: returns the column names of the data frame. \n",
        "- dtypes: returns the data types of each column. \n",
        "- info(): returns a full summary of each \n",
        "\n",
        "the template is `dataframe.<property or method> `\n",
        "\n",
        "```python\n",
        "# -- example calls -- \n",
        "df.index\n",
        "df.shape\n",
        "df.columns \n",
        "df.dtypes \n",
        "df.info()\n",
        "\n",
        "msft.index\n",
        "msft.shape\n",
        "msft.columns\n",
        "msft.info()\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jovmbjbq4nnk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D9lvqbmZ4nnk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T3YQJpsq4nnk"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UtmO6un04nnl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKdfe3ZU4nnl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AN7tKRiC4nnl"
      },
      "source": [
        "## 3. Data Types aka dtypes \n",
        "\n",
        "The dtypes property will show you the data types that pandas infered from the data. If pandas can't infer the data type from the data it will be stored as an object data type, basically treating it like a string. So things like $100.10 will often be imported as an object not as float to convert it will require some cleaning.  \n",
        "\n",
        "Compoare the dtypes of **mstf** and **aapl**. You'll notice Pandas attempt infer datatypes can be different. why does `msft's` **Date** have `datetime64[ns]` while `aapl` has **object**?\n",
        "\n",
        "```python\n",
        "msft.dtypes\n",
        "aapl.dtypes\n",
        "```\n",
        "\n",
        "https://pbpython.com/pandas_dtypes.html\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ytq7CdEn4nnm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1j7rbEtL4nnm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZ8PEvlU4nnm"
      },
      "source": [
        "### Converting data types \n",
        "\n",
        "How can we fix aapl's Date? The simplest thing to do is simply get it right on import. To do this we can tell python which date we want to \"parse as date\" \n",
        "\n",
        "```python\n",
        "\n",
        "# -- import with parse date function -- \n",
        "aapl = pd.read_csv(\"data/aapl.csv\", parse_dates=['Date'])\n",
        "aapl.dtypes\n",
        "\n",
        "```\n",
        "\n",
        "Alternatively, we can assign the datatypes after the fact. \n",
        "\n",
        "```python\n",
        "\n",
        "# -- import then convert -- \n",
        "aapl = pd.read_csv(\"data/aapl.csv\")\n",
        "aapl[\"Date\"] = pd.to_datetime(aapl[\"Date\"])\n",
        "aapl.dtypes\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RP_WBy784nnm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pko3ASBr4nnm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4I4ib8U14nnm"
      },
      "source": [
        "## 4. Filtering Columns \n",
        "\n",
        "Bracket Bracket! to filter or subset the columns in a dataframe we use `df[[<columns separted by comma >]]` here suppose we want to subset the titanic and churn data sets\n",
        "\n",
        "```python\n",
        "# -- PassengerId, Survived, Name, Age -- \n",
        "titanic_res1 = titanic[[\"PassengerId\", \"Survived\", \"Name\", \"Age\"]]\n",
        "titanic_res1.head()\n",
        "\n",
        "# -- same thing just filter from a list -- \n",
        "column_list = [\"PassengerId\", \"Survived\", \"Name\", \"Age\"]\n",
        "titanic_res2 = titanic[column_list]\n",
        "titanic_res2.head()\n",
        "\n",
        "\n",
        "# -- State, Account Length, Area Code, Phone -- \n",
        "churn_res1 = churn[[\"State\", \"Account Length\", \"Area Code\", \"Phone\"]]\n",
        "churn_res1.head()\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JFpcm4LY4nnn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0B-0Or-4nnn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R697erWC4nnn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F68o07HB4nnn"
      },
      "source": [
        "## 5. Filtering Rows\n",
        "\n",
        "Pandas supports the common operator stuff  \n",
        "- equal to: `df[df[”column”] == “some_value”]`\n",
        "- not equal to: `df[df[”column”] != “some_value”]`\n",
        "- greater than: `df[df[”column”] > some_number ]`  **( ==, !=, >, >=, <, <=)**\n",
        "- In a list `df[df[”column”].isin([\"a list\", \"separated by comma\"])]` \n",
        "- use the squiggly **`“~”`** for not in a list,  `df[~df[”column”].isin(a_list)]`\n",
        "- find nulls use **.isna()** `df[df[\"column\"].isna()]`\n",
        "- find not null use **.notnull()** `df[df[\"column\"].notnull()]`\n",
        "\n",
        "**Multiple-Conditions (&|)**\n",
        "- ampersand **&** operator for AND-ing \n",
        "- pipe **|** operator for OR-ing \n",
        "- Parenthesis to handle order of operatioins \n",
        "\n",
        "Here is an example of AND and OR \n",
        "\n",
        "- AND  : `df[ (df[“column1” != “value”) & (df[“column2”] > some_nbr)]`\n",
        "- OR   : `df[ (df[“column1” != “value”) | (df[“column2”] > some_nbr)]`\n",
        "\n",
        "Lets make some results by select titanic passengers who meet these criteria \n",
        "\n",
        "1. passengers Age older than 50\n",
        "2. passengers Age younger than 40 in Pclass 1 \n",
        "3. passengers who are male and are over 20 Age\n",
        "4. passengers who **did not** embark C or Q and are older than 30\n",
        "5. passengers who female and are under 20 Age\n",
        "\n",
        "```python\n",
        "# res1\n",
        "res1 = titanic[titanic[\"Age\"] > 50]\n",
        "res1[\"Survived\"].value_counts()\n",
        "\n",
        "# res2\n",
        "res2 = titanic[(titanic[\"Age\"] < 40) & (titanic[\"Pclass\"] == 1)]\n",
        "res2[\"Survived\"].value_counts()\n",
        "\n",
        "# res3 \n",
        "res3 = titanic[(titanic[\"Sex\"] == \"male\") & (titanic[\"Age\"] > 20)]\n",
        "res3[\"Survived\"].value_counts()\n",
        "\n",
        "# res4 \n",
        "res4 = titanic[(~titanic[\"Embarked\"].isin([\"C\", \"Q\"])) & (titanic[\"Age\"] > 30)] \n",
        "res4[\"Survived\"].value_counts()\n",
        "\n",
        "# res5 \n",
        "res5 = titanic[(titanic[\"Sex\"] == \"female\") & (titanic[\"Age\"] > 20)]\n",
        "res5[\"Survived\"].value_counts()\n",
        "\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dx_dqDqC4nnn"
      },
      "outputs": [],
      "source": [
        "# < insert res 1 here >"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NnOjrxi_5Pn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k4ECHduc5PsG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1Zz1ALEX5Pwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QWSgecfh5P9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XE1IuMpU4nnn"
      },
      "source": [
        "## 6.Queries\n",
        "\n",
        "An alternative to filtering rows via the dataframe is Dataframe.query(). the .query() method is supposed to be somewhat of a similificatoin for conditional and multi-conditional filtering logic, but like everything opensource it has slight syntax variations. For those familar with SQL it may be more intuative. \n",
        "\n",
        "Query supports the common operator stuff but now you are writing an EXPRESSION \n",
        "\n",
        "`df.query('expression1 and expression2')` notice the single quote surrounds the expressions!  \n",
        "\n",
        "- equal to: `df.query('column == “some_value”')`\n",
        "- not equal to: `df.query('column != “some_value”')`\n",
        "- greater than: `df.query('column > some_value')`  ( ==, !=, >, >=, <, <=)\n",
        "- In a list: `df.query('column == [\"list\",\"value\"]')` \n",
        "- Not in List: `df.query('column != [\"list\",\"value\"]')` \n",
        "- find nulls use  `df.query('column.isnull()', engine='python' )` note the engine=python command. \n",
        "    - common alternative `df.query('column != column', engine='python' )`\n",
        "- find not null use  `df.query('column.notnull()', engine='python' )` note the engine=python command. \n",
        "    - common alternative `df.query('column == column', engine='python' )`\n",
        "\n",
        "with query you can connect Multiple-Conditions with the word **and** and **or**\n",
        "\n",
        "`df.query('column != \"value1\" and column2 > 50', engine='python' )`\n",
        "\n",
        "let's use query to ansewr the following questions about churn\n",
        "\n",
        "1. State equals VA\n",
        "2. State in VA, TX, TN and Account Length > 100 \n",
        "3. Day Mins > 100 and Intl Calls > 2 \n",
        "4. VMail Plan == yes and Int'l Plan == no\n",
        "5. (State in VA, TX, TN and Night Mins > 200) or Account Length > 100 \n",
        "\n",
        "You'll notice a challenge with the poorly formated column names you have to use backticks to quote names with Spaces or starting with numbers and escape (\\) columns containing single quotes or any other special character. \n",
        "\n",
        "\n",
        "```python\n",
        "# query 1 \n",
        "q1 = churn.query('State == \"VA\")\n",
        "q1[\"Churn?\"].value_counts()\n",
        "\n",
        "# query 2\n",
        "q2 = churn.query('State == [\"VA\", \"TX\", \"TN\"] and `Account Length` > 100')\n",
        "q2[\"Churn?\"].value_counts()\n",
        "                 \n",
        "# query 3\n",
        "q3 = churn.query('`Day Mins` > 100 and `Intl Calls` > 2 ')\n",
        "q3[\"Churn?\"].value_counts()         \n",
        "                 \n",
        "# query 4\n",
        "q4 = churn.query('`VMail Plan` == \"yes\" and `Int\\'l Plan` == \"no\" ')\n",
        "q4[\"Churn?\"].value_counts() \n",
        "\n",
        "# query 5\n",
        "q5 = churn.query('(State == [\"VA\", \"TX\", \"TN\"] and `Night Mins` > 200) or `Account Length` > 100')\n",
        "q5[\"Churn?\"].value_counts()\n",
        "                 \n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yrWXW-dA4nno"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nrSAA_1L5VcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vVxuRK5Y5Ve7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PChQ7pno5Vuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tz3nEz_F4nno"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiTRSF3I4nno"
      },
      "source": [
        "## 7. How can i filter both Rows and Columns? \n",
        "\n",
        "Now you know how to filter rows and how to filter columns individually you can put them together like this: \n",
        "\n",
        "Dataframe based filtering is simple as adding a *bracket bracket* after the row filter.  \n",
        "\n",
        "```python\n",
        "rowcol1 = titanic[titanic[\"Age\"] > 50][[\"PassengerId\", \n",
        "                                        \"Survived\", \n",
        "                                        \"Name\", \n",
        "                                        \"Age\"]]\n",
        "print(rowcol1[\"Survived\"].value_counts())\n",
        "rowcol1\n",
        "\n",
        "```\n",
        "\n",
        "Query based filtering is as simple as adding a *bracket bracket* after the query as well. \n",
        "\n",
        "```python\n",
        "rowcol2 = churn.query('State == [\"VA\", \"TX\", \"TN\"] \\\n",
        "                       and `Account Length` > 100')[[\"State\", \n",
        "                                                     \"Account Length\",\n",
        "                                                     \"Churn?\",\n",
        "                                                     \"Area Code\", \n",
        "                                                     \"Phone\"]]\n",
        "print(rowcol2[\"Churn?\"].value_counts())\n",
        "\n",
        "rowcol2\n",
        "\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxFEukWw4nno"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hhATlEA94nno"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEszai9x4nno"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xlUJQYAb4nno"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}